###############################################################################
# translation.py - BLEU + BERTScore í‰ê°€ í¬í•¨ + ai_api.py í†µí•©
###############################################################################
import json
import os
import re
from tqdm import tqdm
import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from bert_score import score
from kiwipiepy import Kiwi

# âœ… ai_api.py ì—°ë™
from ai_api import set_model, translate_ko2en, translate_en2ko, MODEL_INFO

# âœ… ì…ë ¥/ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •
input_file = os.path.join("data", "komt-1810k-test.jsonl")
output_dir = "results"
os.makedirs(output_dir, exist_ok=True)

repeat_num = 2  # âœ… ëª¨ë“  ëª¨ë¸ì— ëŒ€í•´ ë™ì¼í•˜ê²Œ ì‚¬ìš©í•  ìƒ˜í”Œ ìˆ˜

# âœ… punkt ì˜¤ë¥˜ ë°©ì§€ (Colabìš©)
nltk.download("punkt")
nltk.download("averaged_perceptron_tagger")
nltk.download("wordnet")
nltk.data.path.append("/root/nltk_data")

# âœ… í•œêµ­ì–´ í† í¬ë‚˜ì´ì €
kiwi = Kiwi()

def is_korean(text):
    return any('ê°€' <= char <= 'í£' for char in text)

def tokenize(text):
    if is_korean(text):
        return [token.form for token in kiwi.tokenize(text, normalize_coda=True)]
    else:
        return text.lower().split()  # âœ… word_tokenize ëŒ€ì‹  ì•ˆì „í•œ ëŒ€ì²´

def simple_score(reference_text, candidate_text):
    reference_tokens = tokenize(reference_text)
    candidate_tokens = tokenize(candidate_text)
    score_val = sentence_bleu([reference_tokens], candidate_tokens,
                               smoothing_function=SmoothingFunction().method2)
    return score_val

def load_json(filename):
    json_data = []
    with open(filename, "r", encoding="utf-8") as f:
        if os.path.splitext(filename)[1] != ".jsonl":
            json_data = json.load(f)
        else:
            for line in f:
                json_data.append(json.loads(line))
    return json_data

def save_json(json_data, filename, option="a"):
    filename = filename.replace(" ", "_")
    with open(filename, option, encoding="utf-8") as f:
        if filename.endswith(".jsonl"):
            for data in json_data:
                json.dump(data, f, ensure_ascii=False)
                f.write("\n")
        else:
            json.dump(json_data, f, ensure_ascii=False, indent=4)

def run_translation(json_data, model_name, repeat_num=1, output_prefix="results", result_collector=None):
    set_model(model_name)

    for repeat in range(repeat_num):
        print(f"\n[Model: {model_name}] ë°˜ë³µ {repeat+1}/{repeat_num}")
        for index, data in tqdm(enumerate(json_data), total=len(json_data)):
            chat = data["conversations"]
            src = data["src"]
            input_text = chat[0]["value"]
            reference = chat[1]["value"]

            def clean_text(text):
                if "í•œê¸€ë¡œ ë²ˆì—­í•˜ì„¸ìš”." in text:
                    cur_lang = "en"
                else:
                    cur_lang = "ko"
                return text.split("ë²ˆì—­í•˜ì„¸ìš”.\n", 1)[-1], cur_lang

            input_text, cur_lang = clean_text(input_text)

            if cur_lang == "en":
                generation = translate_en2ko(input_text)
            else:
                generation = translate_ko2en(input_text)

            # âœ… BLEU ê³„ì‚°
            bleu = round(simple_score(reference, generation), 3)

            # âœ… BERTScore ê³„ì‚°
            try:
                P, R, F1 = score([generation], [reference], lang="ko", model_type="bert-base-multilingual-cased")
                precision = round(P[0].item(), 4)
                recall = round(R[0].item(), 4)
                f1 = round(F1[0].item(), 4)
            except Exception as e:
                print(f"[ERROR] BERTScore ê³„ì‚° ì‹¤íŒ¨: {e}")
                precision = recall = f1 = 0.0

            result = {
                "repeat": repeat + 1,
                "index": index,
                "reference": reference,
                "generation": generation,
                "bleu": bleu,
                "bertscore_precision": precision,
                "bertscore_recall": recall,
                "bertscore_f1": f1,
                "lang": cur_lang,
                "model": model_name,
                "src": src,
                "conversations": chat
            }

            print(json.dumps(result, ensure_ascii=False, indent=2))

            output_file = os.path.join(output_dir, f"{output_prefix}_{model_name}.jsonl")
            save_json([result], output_file)

            # âœ… ê²°ê³¼ ìˆ˜ì§‘
            if result_collector is not None:
                result_collector.append(result)

import matplotlib.pyplot as plt

def plot_model_scores(summary_list):
    models = [s["model"] for s in summary_list]
    bleu_scores = [s["average_bleu"] for s in summary_list]
    f1_scores = [s["average_bertscore_f1"] for s in summary_list]

    x = range(len(models))

    plt.figure(figsize=(12, 6))

    # BLEU
    plt.subplot(1, 2, 1)
    plt.bar(x, bleu_scores)
    plt.xticks(x, models, rotation=45, ha='right')
    plt.title("Average BLEU Score by Model")
    plt.ylabel("BLEU Score")

    # BERTScore F1
    plt.subplot(1, 2, 2)
    plt.bar(x, f1_scores, color='orange')
    plt.xticks(x, models, rotation=45, ha='right')
    plt.title("Average BERTScore F1 by Model")
    plt.ylabel("BERTScore F1")

    plt.tight_layout()
    plt.show()

def main():
    json_data = load_json(input_file)
    all_model_results = {}

    for model_name in MODEL_INFO:
        all_results = []
        run_translation(json_data, model_name, repeat_num=repeat_num, result_collector=all_results)
        all_model_results[model_name] = all_results

    print("\n\nğŸ“Š === ëª¨ë¸ë³„ í‰ê·  í‰ê°€ ê²°ê³¼ ===")
    summaries = []  # âœ… ì €ì¥ìš©

    for model_name, results in all_model_results.items():
        if not results:
            continue
        avg_bleu = sum(r["bleu"] for r in results) / len(results)
        avg_precision = sum(r["bertscore_precision"] for r in results) / len(results)
        avg_recall = sum(r["bertscore_recall"] for r in results) / len(results)
        avg_f1 = sum(r["bertscore_f1"] for r in results) / len(results)

        summary = {
            "model": model_name,
            "average_bleu": round(avg_bleu, 4),
            "average_bertscore_precision": round(avg_precision, 4),
            "average_bertscore_recall": round(avg_recall, 4),
            "average_bertscore_f1": round(avg_f1, 4)
        }

        summaries.append(summary)
        print(json.dumps(summary, ensure_ascii=False, indent=2))

    # âœ… results.jsonlë¡œ ì €ì¥
    save_json(summaries, os.path.join(output_dir, "results.jsonl"), option="w")

    # âœ… ì‹œê°í™”
    plot_model_scores(summaries)


if __name__ == "__main__":
    main()
